<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="LiveOIBench: Can Large Language Models Outperform Human Contestants in Informatics Olympiads?">
  <meta name="keywords" content="LiveOIBench, Large Language Models, Informatics Olympiads, Programming Contest">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LiveOIBench: Can Large Language Models Outperform Human Contestants in Informatics Olympiads?</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/whitebackground.ico" type="image/png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><img src="./static/images/liveoibench_logo.png" alt="LiveOIBench logo" style="height:2em; vertical-align: middle; margin-right: 0.3em;">LiveOIBench: Can Large Language Models Outperform Human Contestants in Informatics Olympiads?</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Kaijian Zou,</span>
            <span class="author-block">
              Aaron Xiong,</span>
            <span class="author-block">
              Yunxiang Zhang,</span>
            <span class="author-block">
              Xinliang Frederick Zhang,</span>
            <span class="author-block">
              Yueqi Ren,</span>
            <span class="author-block">
              Jirong Yang,</span>
            <span class="author-block">
              Ayoung Lee,</span>
            <span class="author-block">
              Shitanshu Bhushan,</span>
            <span class="author-block">
              Lu Wang
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">University of Michigan</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="under-construction.html"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="under-construction.html"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="under-construction.html"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
              </span>
              <!-- Leaderboard Link. -->
              <span class="link-block">
                <a href="leaderboard.html"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa fa-trophy"></i>
                  </span>
                  <span>Leaderboard</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Competitive programming problems increasingly serve as valuable benchmarks to evaluate the coding capabilities of large language models (LLMs) due to their complexity and ease of verification. Yet, current coding benchmarks face limitations such as lack of exceptionally challenging problems, insufficient test case coverage, reliance on online platform APIs that limit accessibility. To address these issues, we introduce LiveOIBench, a comprehensive benchmark featuring 403 expert-curated Olympiad-level competitive programming problems, each with an average of 60 expert-designed test cases. The problems are sourced directly from 72 official Informatics Olympiads in different regions conducted between 2023 and 2025.
          </p>
          <p>
            LiveOIBench distinguishes itself through four key features: (1) meticulously curated high-quality tasks with detailed subtask rubrics and extensive private test cases; (2) direct integration of elite contestant performance data to enable informative comparison against top-performing human; (3) planned continuous, contamination-free updates from newly released Olympiad problems; and (4) a self-contained evaluation system facilitating offline and easy-to-reproduce assessments.
          </p>
          <p>
            Benchmarking 32 popular general-purpose and reasoning LLMs, we find that GPT-5 achieves a notable 81st percentile, a strong result that nonetheless falls short of top human contestant performance, who usually place above 90th. In contrast, among open-weight reasoning models, GPT-OSS-120B achieves only a 60th percentile, underscoring significant capability disparities from frontier closed models. Detailed analyses indicate that robust reasoning models prioritize precise problem analysis over excessive exploration, suggesting future models should emphasize structured analysis and minimize unnecessary exploration.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- LiveOIBench Construction. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">LiveOIBench Construction</h2>
        <div class="content has-text-justified">
          <p>
            Our benchmark consists of 403 coding problems collected directly from the official websites of 72 competitions across 14 renowned Informatics Olympiads, focusing on contests held from 2023 onward. To build this benchmark, we developed custom web crawlers tailored to each official competition website, complemented by thorough manual verification to ensure high accuracy and integrity of the collected data. The dataset includes challenging problems designed by experts, each with clearly defined subtasks that enable precise partial scoring. Additionally, expert-designed private test cases are included to guarantee reliable evaluation. The dataset is further enriched with detailed algorithm tags, clearly marked difficulty levels, and contestant Codeforces ratings, facilitating comprehensive performance analyses. Our benchmark uniquely supports direct comparisons between models and human contestants through percentile rankings, medals, and ratings. To keep the dataset relevant and uncontaminated, we continuously incorporate new contests directly from the official websites of actively maintained competitions.
          </p>
        </div>
      </div>
    </div>
    <!--/ LiveOIBench Construction. -->

    <!-- Models vs. Humans. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Models vs. Humans</h2>
        <div class="content has-text-justified">
          <div class="has-text-centered">
            <img src="./static/images/tokens_vs_percentile.png" alt="Models vs. Humans" style="max-width: 100%; height: auto;">
          </div>
          <p>
            This figure show average human percentile across all contests of each model against
            completion tokens per problem. Our comprehensive benchmarking of 32 LLMs demonstrates
            that proprietary models like GPT-5 currently lead in competitive coding performance, achieving
            exceptional token efficiency and outperforming approximately 82% of human contestants.
            However, GPT-5 still falls short of top-tier human competitors, highlighting a clear performance
            gap at the elite level. Open-weight "thinking" models such as GPT-OSS-120B significantly
            narrow this gap, especially under extended reasoning budgets, while non-thinking models
            consistently lag, underscoring the critical importance of extended reasoning. Additionally,
            sequential scaling strategies further enhance smaller models' capabilities, emphasizing
            inference-time approaches' effectiveness in bridging performance gaps. A multifaceted
            evaluation approach—including human percentile, medal rankings, and Codeforces ELO—provides deeper insights into these nuanced differences in model proficiency. Full results
            can be viewed at our leaderboard page.
          </p>
        </div>
      </div>
    </div>
    <!--/ Models vs. Humans. -->

    <!-- Reasoning Behavior Analysis. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Reasoning Behavior Analysis</h2>
        <div class="content has-text-justified">
          <div class="has-text-centered">
            <img src="./static/images/result.png" alt="Reasoning Behavior Analysis" style="max-width: 100%; height: auto;">
          </div>
          <p>
            The figure provides an analysis of how reasoning behaviors in LLMs
            correlate with problem difficulty, reasoning budgets, model capabilities, and solution
            correctness. As problem difficulty increases (a), GPT-OSS-120B-High prioritizes exploration and
            deeper analysis, reducing initial planning and verification efforts. Conversely, when reasoning
            budgets increase (b), the model strategically directs additional resources toward deeper
            analysis, implementation, and verification without increasing exploration. Stronger reasoning
            models (c) further demonstrate reduced exploration, allocating more tokens to structured
            planning, detailed analysis, and solution development. Finally, correct solutions (d) prominently
            feature structured planning and targeted verification steps, significantly reducing the need for
            continuous re-analysis and exploration, highlighting the importance of robust initial reasoning
            frameworks.
          </p>
        </div>
      </div>
    </div>
    <!--/ Reasoning Behavior Analysis. -->
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{zou2025liveoibench,
  title={LiveOIBench: Can Large Language Models Outperform Human Contestants in Informatics Olympiads?},
  author={Zou, Kaijian and Xiong, Aaron and Zhang, Yunxiang and Zhang, Xinliang Frederick and Ren, Yueqi and Yang, Jirong and Lee, Ayoung and Bhushan, Shitanshu and Wang, Lu},
  year={2025}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            The source code from this website is borrowed from <a
              href="https://github.com/nerfies/nerfies.github.io">this template</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
