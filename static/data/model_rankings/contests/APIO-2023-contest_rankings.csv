Model,Total Score,Included Tasks,Included Score,Avg Tests Passed,Pass Rate (%),Medal,Human Relative Rank,Human Percentile,Rank,Relative Score (%)
gemini-2.5-pro,203.0,3,203.0,78.08,0.0,Silver,54,75.68807339449542,1,67.67
gemini-2.5-flash,144.0,3,144.0,67.98,0.0,None,106,51.8348623853211,2,48.0
gpt-5,132.0,3,132.0,59.68,33.33,None,130,40.825688073394495,3,44.0
deepseek-chat,120.0,3,120.0,60.09,0.0,None,141,35.77981651376147,4,40.0
seed-oss_16384,113.0,3,113.0,44.78,0.0,None,143,34.862385321100916,5,37.67
gpt-4.1,112.0,3,112.0,44.73,0.0,None,143,34.862385321100916,6,37.33
gpt-o3-mini-high,109.0,3,109.0,37.82,0.0,None,143,34.862385321100916,7,36.33
Qwen3-32B-Non-Thinking,108.0,3,108.0,38.55,0.0,None,143,34.862385321100916,8,36.0
DeepSeek-R1-Distill-Qwen-14B,97.0,3,97.0,28.47,0.0,None,149,32.11009174311926,9,32.33
deepseek-reasoner,97.0,3,97.0,30.39,0.0,None,149,32.11009174311926,10,32.33
seed-oss_4096,97.0,3,97.0,30.39,0.0,None,149,32.11009174311926,11,32.33
seed-oss_-1,97.0,3,97.0,37.34,0.0,None,149,32.11009174311926,12,32.33
DeepSeek-R1-Distill-Llama-70B,97.0,3,97.0,28.07,0.0,None,149,32.11009174311926,13,32.33
Qwen3-32B,97.0,3,97.0,56.01,0.0,None,149,32.11009174311926,14,32.33
seed-oss_0,63.0,3,63.0,20.63,0.0,None,181,17.431192660550458,15,21.0
DeepSeek-R1-Distill-Qwen-32B,55.0,3,55.0,26.35,0.0,None,187,14.678899082568808,16,18.33
seed-oss_8192,53.0,3,53.0,29.2,0.0,None,192,12.385321100917432,17,17.67
seed-oss_512,44.0,3,44.0,11.79,0.0,None,194,11.46788990825688,18,14.67
gpt-oss-20b-high,37.0,3,37.0,26.32,0.0,None,197,10.091743119266056,19,12.33
DeepSeek-Coder-V2-Lite-Instruct,35.0,3,35.0,28.03,0.0,None,197,10.091743119266056,20,11.67
gpt-oss-120b-high,31.0,3,31.0,22.42,0.0,None,199,9.174311926605505,21,10.33
Qwen3-8B,31.0,3,31.0,22.7,0.0,None,199,9.174311926605505,22,10.33
Qwen3-14B-Non-Thinking,31.0,3,31.0,23.1,0.0,None,199,9.174311926605505,23,10.33
Qwen3-8B-Non-Thinking,31.0,3,31.0,22.7,0.0,None,199,9.174311926605505,24,10.33
Codestral-22B-v0.1,26.0,3,26.0,22.29,0.0,None,201,8.256880733944953,25,8.67
Qwen2.5-Coder-32B-Instruct,26.0,3,26.0,21.88,0.0,None,201,8.256880733944953,26,8.67
gpt-oss-20b-low,24.0,3,24.0,18.67,0.0,None,203,7.339449541284404,27,8.0
Qwen3-30B-Non-Thinking,24.0,3,24.0,19.08,0.0,None,203,7.339449541284404,28,8.0
gpt-oss-120b-low,24.0,3,24.0,15.8,0.0,None,203,7.339449541284404,29,8.0
Qwen3-30B,20.0,3,20.0,13.72,0.0,None,206,5.963302752293578,30,6.67
QwQ-32B,20.0,3,20.0,13.34,0.0,None,206,5.963302752293578,31,6.67
Qwen3-14B,20.0,3,20.0,13.72,0.0,None,206,5.963302752293578,32,6.67
Qwen3-4B,20.0,3,20.0,13.72,0.0,None,206,5.963302752293578,33,6.67
seed-oss_1024,20.0,3,20.0,12.97,0.0,None,206,5.963302752293578,34,6.67
seed-oss_2048,20.0,3,20.0,8.55,0.0,None,206,5.963302752293578,35,6.67
Llama-3.1-8B-Instruct,15.0,3,15.0,6.5,0.0,None,206,5.963302752293578,36,5.0
gpt-oss-120b-medium,15.0,3,15.0,12.53,0.0,None,206,5.963302752293578,37,5.0
gpt-oss-20b-medium,15.0,3,15.0,15.33,0.0,None,206,5.963302752293578,38,5.0
Llama-4-Scout,15.0,3,15.0,20.31,0.0,None,206,5.963302752293578,39,5.0
Mistral-Small-3.1-24B-2503,15.0,3,15.0,11.4,0.0,None,206,5.963302752293578,40,5.0
Qwen2.5-72B,11.0,3,11.0,14.97,0.0,None,207,5.504587155963303,41,3.67
Qwen3-4B-Non-Thinking,11.0,3,11.0,22.46,0.0,None,207,5.504587155963303,42,3.67
Qwen2.5-Coder-14B-Instruct,11.0,3,11.0,15.38,0.0,None,207,5.504587155963303,43,3.67
Qwen2.5-Coder-7B-Instruct,11.0,3,11.0,15.38,0.0,None,207,5.504587155963303,44,3.67
Llama-3.3-70B-Instruct,11.0,3,11.0,15.78,0.0,None,207,5.504587155963303,45,3.67
DeepSeek-R1-Distill-Llama-8B,4.0,3,4.0,5.33,0.0,None,210,4.128440366972477,46,1.33
Mistral-Large-Instruct-2411,0.0,3,0.0,0.0,0.0,None,210,4.128440366972477,47,0.0
DeepSeek-R1-Distill-Qwen-7B,0.0,3,0.0,0.0,0.0,None,210,4.128440366972477,48,0.0
